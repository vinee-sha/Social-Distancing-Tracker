{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HumanDetection.ipynb",
      "provenance": [],
      "mount_file_id": "1MncA-48hovF5RyIH0bgXUraQZ-tVGXBs",
      "authorship_tag": "ABX9TyPwwGreSeHLpYLNny+quxSg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinee-sha/Social-Distancing-Tracker/blob/main/HumanDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5Wjc8JR86a0"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from keras import models"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZFJwA-T8hEr",
        "outputId": "c421bc01-cb47-4cd5-f897-3a715a2999fc"
      },
      "source": [
        "#Load the saved model\n",
        "model = models.load_model('/content/drive/MyDrive/human_model.h5')\n",
        "video = cv2.VideoCapture(\"/content/img.jpeg\")\n",
        "\n",
        "while True:\n",
        "        success, frame = video.read()\n",
        "        #print(frame.shape)\n",
        "        # print(success)\n",
        "        # print(frame)\n",
        "        # Convert the captured frame into RGB\n",
        "        # im = Image.fromarray(frame, 'RGB')\n",
        "        # print(im)\n",
        "        #Resizing into 128x128 because we trained the model with this image size.\n",
        "        im = cv2.resize(frame, (150, 150))\n",
        "        #print(\"After resize\",im)\n",
        "        #print(im.shape)\n",
        "        img_array = np.array(im)\n",
        "        #print(\"Array\", img_array)\n",
        "  \n",
        "        # #Our keras model used a 4D tensor, (images x height x width x channel)\n",
        "        # #So changing dimension 128x128x3 into 1x128x128x3 \n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        #print(img_array.shape)\n",
        "        print(model.predict(img_array))\n",
        "        break\n",
        "        # #Calling the predict method on model to predict 'me' on the image\n",
        "        #prediction = int(model.predict(img_array)[0][0])\n",
        "\n",
        "\n",
        "        # #if prediction is 0, which means I am missing on the image, then show the frame in gray color.\n",
        "        # if prediction == 0:\n",
        "        #         frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # cv2.imshow(\"Capturing\", frame)\n",
        "        # key=cv2.waitKey(1)\n",
        "        # if key == ord('q'):\n",
        "        #         break\n",
        "video.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pPiHE0L82Rf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}